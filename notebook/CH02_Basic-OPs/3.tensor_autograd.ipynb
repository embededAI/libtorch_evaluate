{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "circular-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include \"stdio.h\"\n",
    "#include \"stdlib.h\"\n",
    "#include <iostream>\n",
    "#include <vector>  \n",
    "\n",
    "/*a workaround to solve cling issue*/\n",
    "#include \"../macos_cling_workaround.hpp\"\n",
    "/*set libtorch path, load libs*/\n",
    "#include \"../load_libtorch.hpp\"\n",
    "/*import custom defined macros*/\n",
    "#include \"../custom_def.hpp\"\n",
    "/*import libtorch header file*/\n",
    "#include <torch/torch.h>\n",
    "\n",
    "std::cout << std::boolalpha;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-cocktail",
   "metadata": {},
   "source": [
    "> ***自动求梯度是PyTorch非常有特色的一项功能，不论是机器学习还是深度学习，我们经常需要对函数求梯度，特别是深度神经网络在执行反向传播操作时，求梯度是必备功能之一。***\n",
    "\n",
    "> ***PyTorch提供的autograd包能够根据输入和前向传播过程自动构建计算图，并执行反向传播。***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-irrigation",
   "metadata": {},
   "source": [
    "# 1. 简单示例\n",
    "\n",
    "PyTorch(libtorch)提供的autograd功能是围绕着torch::Tensor来的,如果将tensor的属性.requires_grad设置为True，它将开始追踪(track)在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后，可以调用.backward()来完成所有梯度计算。此Tensor的梯度将累积到.grad属性中。如果不想要被继续追踪，可以调用.detach()将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用`torch::NoGradGuard no_grad;` 将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（requires_grad=True）的梯度。\n",
    "\n",
    "Function是另外一个很重要的类。Tensor和Function互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。每个Tensor都有一个.grad_fn属性，该属性即创建该Tensor的Function, 就是说该Tensor是不是通过某些运算得到的，若是，则grad_fn返回一个与这些运算相关的对象，否则是None。\n",
    "\n",
    "下面先通过一些例子看看autograd功能是如何使用的(注：此处我们并不完全按照“dive into DL pytorch”教程中的方式来，而是结合了[官方教程](https://pytorch.org/tutorials/advanced/cpp_autograd.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pacific-dating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = \n",
      " 1  1\n",
      " 1  1\n",
      "[ CPUFloatType{2,2} ]\n",
      "<<--->>\n",
      "\n",
      "x.grad_fn()->name() = \n",
      "AddBackward1\n",
      "<<--->>\n",
      "\n",
      "y = \n",
      " 27  27\n",
      " 27  27\n",
      "[ CPUFloatType{2,2} ]\n",
      "<<--->>\n",
      "\n",
      "y.grad_fn()->name() = \n",
      "MulBackward1\n",
      "<<--->>\n",
      "\n",
      "out = \n",
      "27\n",
      "[ CPUFloatType{} ]\n",
      "<<--->>\n",
      "\n",
      "out.grad_fn()->name() = \n",
      "MeanBackward0\n",
      "<<--->>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//创建一个Tensor并设置requires_grad=true去跟踪其计算过程:\n",
    "torch::Tensor x = torch::ones({2,2}, torch::requires_grad(true));\n",
    "printT(x);\n",
    "\n",
    "x = x+2;\n",
    "//对于tensor加法而言，其梯度函数为：AddBackward1\n",
    "printT(x.grad_fn()->name());\n",
    "\n",
    "//如果上面参数torch::requires_grad设置为false，则上面语句会报错：\n",
    "//null passed to a callee that requires a non-null argument [-Wnonnull]\n",
    "\n",
    "// 下面再看几个运算符的反向传播函数名字\n",
    "auto y = x * x *3;\n",
    "auto out = y.mean();\n",
    "\n",
    "printT(y);\n",
    "printT(y.grad_fn()->name());\n",
    "\n",
    "printT(out);\n",
    "printT(out.grad_fn()->name());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-assembly",
   "metadata": {},
   "source": [
    "***初始化Tensor后再修改其autograd属性：***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hungry-sheffield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.requires_grad() = \n",
      "false\n",
      "<<--->>\n",
      "\n",
      "a.requires_grad() = \n",
      "true\n",
      "<<--->>\n",
      "\n",
      "b.grad_fn()->name() = \n",
      "SumBackward0\n",
      "<<--->>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/* ***********************************************\n",
    " * 新创建的Tensor是默认不支持autograd属性的，\n",
    " * 如下面这个例子，实际运行时会报错：\n",
    " *\n",
    " * torch::Tensor a = torch::ones({2,2});\n",
    " * auto b = a*3;\n",
    " * std::cout << b.grad_fn()->name() << std::endl;\n",
    " * \n",
    " * 但是可以使用requires_grad_()这个内建函数来改变：\n",
    " * a.requires_grad_(true);\n",
    " ************************************************* */\n",
    "\n",
    "auto a = torch::randn({2, 2});\n",
    "a = ((a * 3) / (a - 1));\n",
    "printT(a.requires_grad());\n",
    "\n",
    "a.requires_grad_(true);\n",
    "printT(a.requires_grad());\n",
    "\n",
    "auto b = (a * a).sum();\n",
    "printT(b.grad_fn()->name());\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-regard",
   "metadata": {},
   "source": [
    "***用tensor的内建函数backward()求梯度***\n",
    "\n",
    "我们来看看out关于x的梯度 $\\frac{d(out)}{dx}$，\n",
    "我们令out为 $o$ , 因为 $$ o=\\frac14\\sum_{i=1}^4z_i=\\frac14\\sum_{i=1}^43(x_i+2)^2 $$ 所以 $$ \\frac{\\partial{o}}{\\partial{x_i}}\\bigr\\rvert_{x_i=1}=\\frac{9}{2}=4.5 $$ 所以上面的输出是正确的。\n",
    "\n",
    "数学上，如果有一个函数值和自变量都为向量的函数 $\\vec{y}=f(\\vec{x})$, 那么 $\\vec{y}$ 关于 $\\vec{x}$ 的梯度就是一个雅可比矩阵（Jacobian matrix）: $$ J=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\ \\vdots & \\ddots & \\vdots\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right) $$ 而torch.autograd这个包就是用来计算一些雅克比矩阵的乘积的。例如，如果 $v$ 是一个标量函数的 $l=g\\left(\\vec{y}\\right)$ 的梯度： $$ v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right) $$ 那么根据链式法则我们有 $l$ 关于 $\\vec{x}$ 的雅克比矩阵就为: $$ v J=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right) \\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\ \\vdots & \\ddots & \\vdots\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial x_{1}} & \\cdots & \\frac{\\partial l}{\\partial x_{n}}\\end{array}\\right) $$\n",
    "\n",
    "注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "compressed-poultry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad() = \n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[ CPUFloatType{2,2} ]\n",
      "<<--->>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//再回到刚开始那个例子，给out变量做一次反向传播运算，看看其梯度是多少\n",
    "torch::Tensor x = torch::ones({2,2}, torch::requires_grad(true));\n",
    "auto y = x+2;\n",
    "auto z = y * y *3;\n",
    "auto out = z.mean();\n",
    "//求梯度，只能针对scalar类型的变量进行\n",
    "out.backward();\n",
    "\n",
    "printT(x.grad());\n",
    "//梯度是累加的，求完后需要清理一下\n",
    "x.grad().data().zero_();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "activated-timing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = \n",
      " 1.6286\n",
      "-5.3786\n",
      " 1.0283\n",
      "[ CPUFloatType{3} ]\n",
      "<<--->>\n",
      "\n",
      "y.grad_fn()->name() = \n",
      "MulBackward1\n",
      "<<--->>\n",
      "\n",
      "x.grad() = \n",
      " 0.2000\n",
      " 2.0000\n",
      " 0.0002\n",
      "[ CPUFloatType{3} ]\n",
      "<<--->>\n",
      "\n",
      "true\n",
      "false\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "torch::Tensor x = torch::randn(3, torch::requires_grad());\n",
    "\n",
    "torch::Tensor y = x * 2;\n",
    "// while (y.norm().item<double>() < 1000) {\n",
    "//   y = y * 2;\n",
    "// }\n",
    "\n",
    "printT(y);\n",
    "printT(y.grad_fn()->name());\n",
    "\n",
    "auto v = torch::tensor({0.1, 1.0, 0.0001}, torch::kFloat);\n",
    "y.backward(v);\n",
    "\n",
    "printT(x.grad());\n",
    "\n",
    "std::cout << x.requires_grad() << std::endl;\n",
    "y = x.detach();\n",
    "std::cout << y.requires_grad() << std::endl;\n",
    "std::cout << x.eq(y).all().item<bool>() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-console",
   "metadata": {},
   "source": [
    "# 2.复杂点的例子，构造自己的前向/反向传播函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-zimbabwe",
   "metadata": {},
   "source": [
    "在上面一节里，我们通过一些基本的算术运算来演示了反向传播计算的过程，下面通过构造一个线性运算类来演示如何自行编写前向、反向传播函数；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lovely-class",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.8849 -0.5697  3.1114\n",
      " 0.8849 -0.5697  3.1114\n",
      "[ CPUFloatType{2,3} ]\n",
      " 2.7916  0.3543 -1.0179\n",
      " 2.7916  0.3543 -1.0179\n",
      " 2.7916  0.3543 -1.0179\n",
      " 2.7916  0.3543 -1.0179\n",
      "[ CPUFloatType{4,3} ]\n",
      "torch::autograd::CppNode<LinearFunction>\n"
     ]
    }
   ],
   "source": [
    "using namespace torch::autograd;\n",
    "\n",
    "// Inherit from Function\n",
    "class LinearFunction : public Function<LinearFunction> {\n",
    " public:\n",
    "  // Note that both forward and backward are static functions\n",
    "\n",
    "  // bias is an optional argument\n",
    "  static torch::Tensor forward(\n",
    "      AutogradContext *ctx, torch::Tensor input, torch::Tensor weight, torch::Tensor bias = torch::Tensor()) {\n",
    "    ctx->save_for_backward({input, weight, bias});\n",
    "    auto output = input.mm(weight.t());\n",
    "    if (bias.defined()) {\n",
    "      output += bias.unsqueeze(0).expand_as(output);\n",
    "    }\n",
    "    return output;\n",
    "  }\n",
    "\n",
    "  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n",
    "    auto saved = ctx->get_saved_variables();\n",
    "    auto input = saved[0];\n",
    "    auto weight = saved[1];\n",
    "    auto bias = saved[2];\n",
    "\n",
    "    auto grad_output = grad_outputs[0];\n",
    "    auto grad_input = grad_output.mm(weight);\n",
    "    auto grad_weight = grad_output.t().mm(input);\n",
    "    auto grad_bias = torch::Tensor();\n",
    "    if (bias.defined()) {\n",
    "      grad_bias = grad_output.sum(0);\n",
    "    }\n",
    "\n",
    "    return {grad_input, grad_weight, grad_bias};\n",
    "  }\n",
    "};\n",
    "\n",
    "\n",
    "////////////////////////////////////////////////////////\n",
    "//\n",
    "////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "auto x = torch::randn({2, 3}).requires_grad_();\n",
    "auto weight = torch::randn({4, 3}).requires_grad_();\n",
    "auto y = LinearFunction::apply(x, weight);\n",
    "y.sum().backward();\n",
    "\n",
    "std::cout << x.grad() << std::endl;\n",
    "std::cout << weight.grad() << std::endl;\n",
    "\n",
    "std::cout << y.grad_fn()->name() << std::endl;\n",
    "\n",
    "y.detach();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-millennium",
   "metadata": {},
   "source": [
    "### Python API与C++ API对照表\n",
    "\n",
    "\n",
    "| Python                       | C++                                                          |\n",
    "| :---------------------------- | :------------------------------------------------------------ |\n",
    "| `torch.autograd.backward`    | `torch::autograd::backward` ([link](https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1afa9b5d4329085df4b6b3d4b4be48914b.html)) |\n",
    "| `torch.autograd.grad`        | `torch::autograd::grad` ([link](https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1a1e03c42b14b40c306f9eb947ef842d9c.html)) |\n",
    "| `torch.Tensor.detach`        | `torch::Tensor::detach` ([link](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor6detachEv)) |\n",
    "| `torch.Tensor.detach_`       | `torch::Tensor::detach_` ([link](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7detach_Ev)) |\n",
    "| `torch.Tensor.backward`      | `torch::Tensor::backward` ([link](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor8backwardERK6Tensorbb)) |\n",
    "| `torch.Tensor.register_hook` | `torch::Tensor::register_hook` ([link](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4I0ENK2at6Tensor13register_hookE18hook_return_void_tI1TERR1T)) |\n",
    "| `torch.Tensor.requires_grad` | `torch::Tensor::requires_grad_` ([link](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor14requires_grad_Eb)) |\n",
    "| `torch.Tensor.retain_grad`   | `torch::Tensor::retain_grad` ([link](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor11retain_gradEv)) |\n",
    "| `torch.Tensor.grad`          | `torch::Tensor::grad` ([link](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor4gradEv)) |\n",
    "| `torch.Tensor.grad_fn`       | `torch::Tensor::grad_fn` ([link](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7grad_fnEv)) |\n",
    "| `torch.Tensor.set_data`      | `torch::Tensor::set_data` ([link](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor8set_dataERK6Tensor)) |\n",
    "| `torch.Tensor.data`          | `torch::Tensor::data` ([link](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor4dataEv)) |\n",
    "| `torch.Tensor.output_nr`     | `torch::Tensor::output_nr` ([link](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor9output_nrEv)) |\n",
    "| `torch.Tensor.is_leaf`       | `torch::Tensor::is_leaf` ([link](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7is_leafEv)) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-component",
   "metadata": {},
   "source": [
    "# 3.结合源码，看看autograd的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-dominant",
   "metadata": {},
   "source": [
    "通常，我们采用下述方式创建支持autograd功能的tensor："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liberal-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch::Tensor t1 = torch::empty({2,2}, torch::requires_grad());\n",
    "\n",
    "//or\n",
    "\n",
    "torch::Tensor t2 = torch::empty({2,2}, torch::requires_grad(true));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-andrew",
   "metadata": {},
   "source": [
    "问题来了，这个requires_grad(...)是哪里来的？ \n",
    "看下面代码片段可知，其来自TensorOptions.h:\n",
    "\n",
    "```\n",
    "/// Convenience function that returns a `TensorOptions` object with the\n",
    "/// `requires_grad` set to the given one.\n",
    "inline TensorOptions requires_grad(bool requires_grad = true) {\n",
    "    return TensorOptions().requires_grad(requires_grad);\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "即， `torch::requires_grad(...)` 返回了一个TensorOptions对象，且其requires_grad_属性默认是true；\n",
    "\n",
    "顺着上述代码，可以看到TensorOptions的requires_grad(...)函数：\n",
    "\n",
    "``` \n",
    "/// Sets the `requires_grad` property of the `TensorOptions`.\n",
    "C10_NODISCARD TensorOptions requires_grad(c10::optional<bool> requires_grad) const noexcept { \n",
    "  TensorOptions r = *this;\n",
    "  r.set_requires_grad(requires_grad);\n",
    "  return r;                                                                                                   }\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "/// Mutably set the `requires_grad` property of `TensorOptions`.\n",
    "void set_requires_grad(c10::optional<bool> requires_grad) & noexcept {                                                                                                                 \n",
    "  if (requires_grad) {\n",
    "    requires_grad_ = *requires_grad;\n",
    "    has_requires_grad_ = true;\n",
    "  } else {\n",
    "    has_requires_grad_ = false;\n",
    "  }   \n",
    "}\n",
    "\n",
    "```\n",
    "鼓捣半天，其实就是给这个TensorOptions对象的requires_grad_属性赋值 ：（\n",
    "显然，这还不是我们想要的结果，我们想看的是tensor对象的autograd实现，继续跟踪源码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-linux",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-express",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-former",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-playback",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-patrol",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
